{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sustainable Software Development, block course, March 2021**  \n",
    "*Scientific Software Center, Institute for Scientific Computing, Dr. Inga Ulusoy*\n",
    "\n",
    "# Analysis of the data\n",
    "\n",
    "Imagine you perform a \"measurement\" of some type and obtain \"scientific data\". You know what your data represents, but you have only a vague idea how different features in the data are connected, and what information you can extract from the data.\n",
    "\n",
    "You would start first with going through the data, making sure your data set is complete and that the result is reasonable. Imagine this already happened.\n",
    "\n",
    "In the next step, you would inspect your data more closely and try to identify structures. That is the step that we are focusing on in this unit.\n",
    "\n",
    "In the `data` folder, you will find several data files (`*.t` and `*.dat`). These are data files generated through some \"new approach\" that hasn't been used in your lab before. No previous analysis software exists, and you are going to establish a protocol for this \"new approach\" and \"publish your results\".\n",
    "\n",
    "The data can be grouped into two categories: \n",
    "1. data to be analyzed using statistical methods;\n",
    "2. data to be analyzed using numerical methods.\n",
    "\n",
    "In your hypothetical lab, you are an \"expert\" in one particular \"method\", and your co-worker is an \"expert\" in the other. Combined these two methods will lead to much more impactful results than if only one of you analyzed the data. Now, the task in this course is to be solved collaboratively with your team member working on one of the analysis approaches, and you working on the other. You will both implement functionality into the same piece of \"software\", but do so collaboratively through git.\n",
    "\n",
    "As you do not know yet which analysis is most meaningful for your data, and how to implement it, you will start with a jupyter notebook. You and your team member will work on the same notebook that will be part of a github repository for your project. This is the task for today. Discuss with your team members who will work on the statistical and who on the numerical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "Generate a github repository with the relevant files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import LCpackage as LC \n",
    "\n",
    "LC.statistical.del_below()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Clone the repository to your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "Start working on task 1 for your analysis approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "Create your own branch of the repository and commit your changes to your branch; push to the remote repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "\n",
    "Open a `pull request` so your team member can review your implementation. Likewise, your team member will ask you to review theirs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6\n",
    "\n",
    "Merge the changes in your branch into `main`. Resolve conflicts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7\n",
    "\n",
    "Repeat working on task; committing and pushing to your previously generated branch or a new branch; open a pull request; merge with main; until you have finished all the tasks in your analysis approach. Delete obsolete branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "\n",
    "df_ = pd.DataFrame(np.array([[1,1,1],[2,3,4],[3,4,5]]), columns=['col1', 'col2', 'col3'], index=['a', 'b', 'c'])\n",
    "\n",
    "print(df_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of the analysis notebook\n",
    "**Author : Your Name**  \n",
    "*Date : The date you started working on this*  \n",
    "*Affiliation : The entity under whose name you are working on this*  \n",
    "\n",
    "Place the required modules in the top, followed by required constants and global functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants and global functions\n",
    "threshv = 1.0e-5\n",
    "filenames = 'efield.t' , 'expec.t', 'npop.t', 'nstate_i.t', 'table.dat' #names of the files to acces later\n",
    "filedir= '../data/'  #directory in which the data is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading of the data files\n",
    "def read_in_df(filedir, filename):\n",
    "    name = '{}{}'.format(filedir, filename)     #what is this? -> creates filedir-filename \"merge\"\n",
    "    print('Reading from file {} - pandas'.format(name))    \n",
    "    data = pd.read_csv(name, r'\\s+')                       #turn csv into dataframe?     sad\n",
    "    return data\n",
    "def read_in_np(filedir, filename):\n",
    "    name = '{}{}'.format(filedir, filename)\n",
    "    print('Reading from file {} - numpy'.format(name))\n",
    "    data = np.loadtxt(name, skiprows=1)\n",
    "    data = data.T\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical analysis\n",
    "\n",
    "Find correlations in the data sets. Analyse the data statistically and plot your results.  \n",
    "\n",
    "Here we would want to do everything with pandas and leave the data in a dataframe. The files that are relevant to you are `expect.t`, `npop.t` and `table.dat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Read in expec.t and plot relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and plot expec.t\n",
    "df_expec = read_in_df(filedir, filenames[1])\n",
    "#print(df_expec.head(10))               \n",
    "sn.relplot(data=df_expec, kind='line', x='time', y='<z>')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can discard the entries norm, \\<x>, and \\<y> as these are mostly constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate columns based on the variance: if the variance of the values\n",
    "# in a column is below a given threshold, that column is discarded\n",
    "\n",
    "def del_below(df,threshhold):\n",
    "    dellist=[]                              #create a list which will hold the columns that have a variance below the threshhold\n",
    "    for i in range(len(df.columns)):        #iterate over all columns\n",
    "        if df.var()[i] < threshhold:        #identify the relevant columns to delete\n",
    "            dellist.append(i)               #add their index to the list\n",
    "    df.drop(df.columns[dellist], axis=1, inplace=True)      #delete the columns\n",
    "    return df                                                   \n",
    "\n",
    "df_expec=del_below(df_expec,threshv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Create plots of the relevant data and save as .pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plots -> just same plot as above?\n",
    "from pathlib import Path\n",
    "\n",
    "def save(df, xaxis, yaxis, path):                        \n",
    "    sn.relplot(data=df, kind='line', x=xaxis, y=yaxis)   \n",
    "    if Path(path).exists() == False:   \n",
    "        plt.savefig(path)\n",
    "    elif Path(path).exists() == True:\n",
    "        print('{} already exists. Overwrite? Y/N'.format(path))\n",
    "        check = input()\n",
    "        if check == 'Y':\n",
    "            plt.savefig(path)\n",
    "            print('File {} was overwriten'.format(path))\n",
    "        else:\n",
    "            print('File {} was not overwriten'.format(path))\n",
    "\n",
    "save(df_expec, 'time', '<z>', '../plots/Relevant_data_task1.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Read in file `npop.t` and analyze correlations in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # read in npop.t\n",
    " df_2=read_in_df(filedir, filenames[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard all columns with variance below a set threshold - we can consider them as constant\n",
    "df_2=del_below(df_2,threshv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the remaining columns. Seaborn prefers \"long format\" (one column for all measurement values, one column to indicate the type) as input, whereas the cvs is in \"wide format\" (one column per measurement type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ideally with seaborn\n",
    "def plot_multiple(df, x, var_name, value_name):    \n",
    "    df_2_melted=df_2.melt(x, var_name=var_name, value_name=value_name)\n",
    "    sn.relplot(data=df_2_melted, kind='line', x=x, y=value_name, hue=var_name)  \n",
    "    plt.savefig('')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantify the pairwise correlation in the data\n",
    "\n",
    "- negative correlation: y values decrease for increasing x - large values of one feature correspond to small values of the other feature\n",
    "- weak or no correlation: no trend observable, association between two features is hardly observable\n",
    "- positive correlation: y values increase for decreasing x - small values of one feature correspond to small values of the other feature\n",
    "\n",
    "Remember that correlation does not indicate causation - the reason that two features are associated can lie in their dependence on same factors.\n",
    "\n",
    "Correlate the value pairs using Pearson's $r$. Pearson's $r$ is a measure of the linear relationship between features:\n",
    "\n",
    "$r = \\frac{\\sum_i(x_i − \\bar{x})(y_i − \\bar{y})}{\\sqrt{\\sum_i(x_i − \\bar{x})^2 \\sum_i(y_i − \\bar{y})^2}}$\n",
    "\n",
    "Here, $\\bar{x}$ and $\\bar{y}$ indicate mean values. $i$ runs over the whole data set. For a positive correlation, $r$ is positive, and negative for a negative correlation, with minimum and maximum values of -1 and 1, indicating a perfectly linear relationship. Weakly or not correlated features are characterized by $r$-values close to 0.\n",
    "\n",
    "Other measures of correlation that can be used are Spearman's rank (value pairs follow monotonic function) or Kendall's $\\tau$ (measures ordinal association), but they do not apply here. You can also define measures yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the correlation matrix\n",
    "#print(df_2.mean())\n",
    "#print(df_2.loc[2, df_2.columns[2]])\n",
    "#print(df_2.loc[2, df_2.columns[3]])\n",
    "\n",
    "#def generate_nullmatrix(rows,columns):          #function to generate a 0 matrix with a specific number of rows and columns,\n",
    "#    _list=[]                                     ended up not using this but leaving it in just in case\n",
    "#    column=[]\n",
    "#    for i in range(columns):\n",
    "#        column.append(0)\n",
    "#    for k in range(rows):\n",
    "#         _list.append(column)\n",
    "#    outp = np.array(_list)\n",
    "#    return outp\n",
    "\n",
    "def upper_sum(df,k,j):                #get the upper value, sum over the entire column k compared to column j\n",
    "    outp=0\n",
    "    for i in range(len(df.index)):\n",
    "        outp = outp + (df.loc[i, df.columns[j]]-df.mean()[j])*(df.loc[i, df.columns[k]]-df.mean()[k])\n",
    "    return outp\n",
    "\n",
    "def lower_sum(df,k,j):                #get the lower value, apparently the length of some vector or smthg\n",
    "    sum1=0                            #again compare given column k to given column j\n",
    "    sum2=0\n",
    "    for i in range(len(df.index)):\n",
    "        sum1 = sum1 + (df.loc[i, df.columns[k]]- df.mean()[k])**2\n",
    "    for i in range(len(df.index)):\n",
    "        sum2 = sum2 + (df.loc[i, df.columns[j]]- df.mean()[j])**2\n",
    "    outp = (sum1*sum2)**(1/2)\n",
    "    return outp\n",
    "\n",
    "def pear_values(df,j): \n",
    "    row_of_values=[]                        #get a row of values for a given first variable, comparing it to all others\n",
    "    for k in range(len(df.columns)): \n",
    "            row_of_values.append(upper_sum(df,j,k)/lower_sum(df,j,k))\n",
    "    return row_of_values\n",
    "\n",
    "def pear_matrix(df):         #pull all rows through pear_values and merge them in one np.array with dimension [columns x columns]\n",
    "    list_of_rows=[]            \n",
    "    for i in range(len(df.columns)):\n",
    "        list_of_rows.append(pear_values(df,i))\n",
    "    matrix = np.array(list_of_rows) # -> this is the correlation matrix\n",
    "    return matrix\n",
    "            \n",
    "#corr_matrix_array = pear_matrix(df_2)\n",
    "#I don't think i got this right :) maybe i did now, whoop whoop!\n",
    "\n",
    "corr_matrix = df_2.corr(method='pearson') #the easy way i guess, considerably better though as you get an idea what was compared\n",
    "                                          #where my method only yields the pure matrix without such information\n",
    "#print(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagonal values tell us that each value is perfectly correlated with itself. We are not interested in the diagonal values and also not in the correlation with time. We also need to get rid of redundant entries. Finally, we need to find the value pairs that exhibit the highest linear correlation. We still want to know if it is positive or negative correlation, so we cannot get rid of the sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of time column, lower triangular and diagonal entries of the correlation matrix\n",
    "#corr_matrix_array = pear_matrix(df_2)\n",
    "\n",
    "#def dropper(matrix):                    #this does exactly what i want it to do, but i lose the information on what was  \n",
    "#    for i in range(len(matrix)):        #represented if i sort now\n",
    "#        for k in range(len(matrix)):\n",
    "#            if i == k: \n",
    "#                matrix[i,k] = 0\n",
    "#            elif k == 0:\n",
    "#                matrix[i,k] = 0\n",
    "#            elif i > k :\n",
    "#                matrix[i,k] = 0\n",
    "#    return matrix\n",
    "def correlation(df):\n",
    "    corr_matrix = df.corr(method='pearson')\n",
    "    print(corr_matrix)\n",
    "    print('Drop lower triangular and diagonal values? Y/N')\n",
    "    check = input()\n",
    "    if check == 'Y':\n",
    "        df_temp = pd.DataFrame(index=corr_matrix.index, columns=corr_matrix.columns)      \n",
    "        for i in range(len(corr_matrix.columns)):                               \n",
    "            for k in range(len(corr_matrix.index)):                             \n",
    "                if i == k:                                              \n",
    "                    df_temp.iloc[i, k] = 0         \n",
    "                elif i > k:\n",
    "                    df_temp.iloc[i, k] = 0\n",
    "                else:\n",
    "                    df_temp.iloc[i, k] = 1\n",
    "        corr_matrix_reduced = corr_matrix*df_temp\n",
    "        print(corr_matrix_reduced)\n",
    "        return corr_matrix_reduced\n",
    "    else:\n",
    "        return corr_matrix \n",
    "\n",
    "def dropper_pandas(df):                                             #set values for diagonal and sub-diagonal elements to 0\n",
    "    df_temp = pd.DataFrame(index=df.index, columns=df.columns)      #tried NaN, but ran into problems excluding these later\n",
    "    for i in range(len(df.columns)):                                #whereas 0 can be easily checked against\n",
    "        for k in range(len(df.index)):                              #the function creates a mask which is 0 for the diagonal\n",
    "            if i == k:                                              #and sub-diagonal entries and 1 everywhere else, than \n",
    "                df_temp.iloc[i, k] = 0         #multiplies input dataframe with mask and returns product\n",
    "            #elif k == 0:\n",
    "             #   df_temp.iloc[i, k] = 0\n",
    "            elif i > k:\n",
    "                df_temp.iloc[i, k] = 0\n",
    "            else:\n",
    "                df_temp.iloc[i, k] = 1\n",
    "    df = df*df_temp\n",
    "    return df\n",
    "\n",
    "#corr_matrix = df_2.corr(method='pearson')\n",
    "#corr_matrix=dropper_pandas(corr_matrix)\n",
    "#print(corr_matrix)\n",
    "\n",
    "test = correlation(df_2)\n",
    "\n",
    "# sort the remaing values according to their absolute value, but keep the sign\n",
    "\n",
    "#how to sort but still now what was correlated? maybe somehow put index and columns name in list together with value and create list of such lists, then sort this list based on the values within the contained lists...\n",
    " \n",
    "def array_of_entries_without_x(df, x):                #this creates an array based on such a list because this lets me understand \n",
    "    list_of_entries = []                              #how to index this catastrophe\n",
    "    for i in range(len(df.index)):\n",
    "        for k in range(len(df.columns)):        \n",
    "            if df.index[i] != x :           #f*** time i guess\n",
    "                if df.columns[k] != x :    \n",
    "                    if df.loc[df.index[i], df.columns[k]] != 0 :        #can't ckeck against NaN apparently...\n",
    "                        list_of_entries.append([df.index[i], df.columns[k], df.iloc[i, k]])\n",
    "    list_of_entries = np.array(list_of_entries)\n",
    "    return list_of_entries\n",
    "\n",
    "\n",
    "def sort_array(array_in, axis, ):               #takes a np.array with n rows and m columns, sorts the entries of the column denominated by\n",
    "    for i in range(1,len(array_in)):          #variable axis based on absolute value using insertion sort, keeps the sign, returns sorted \n",
    "        key_item = float(array_in[i,axis])    #np.array\n",
    "        j = i-1\n",
    "        while j >= 0 and abs(float(array_in[j,axis])) > abs(key_item):\n",
    "            array_in[j+1,axis]=float(array_in[j,axis])\n",
    "            j = j-1\n",
    "        array_in[j+1, axis] = key_item\n",
    "    return array_in\n",
    "    \n",
    "unsorted_array=array_of_entries_without_x(corr_matrix, x = 'time')\n",
    "sorted_array=sort_array(unsorted_array, axis = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values(df, exclude, sort, path):\n",
    "    \"\"\"[Extract values from correlation matrix, write to np.array.]\n",
    "\n",
    "    Args:\n",
    "        df ([DataFrame]): [Correlation Matrix]\n",
    "        exclude ([String]): [Variable whos correlation is to be excluded]\n",
    "        sort ([Boolean]): [Set to true for values to be sorted along a certain axis]\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    list_of_entries = []                              \n",
    "    for i in range(len(df.index)):\n",
    "        for k in range(len(df.columns)):        \n",
    "            if df.index[i] != x :           \n",
    "                if df.columns[k] != x :    \n",
    "                    if df.loc[df.index[i], df.columns[k]] != 0 :        \n",
    "                        list_of_entries.append([df.index[i], df.columns[k], df.iloc[i, k]])\n",
    "    list_of_entries = np.array(list_of_entries)     #list containing what was correlated and related values\n",
    "    array_in = list_of_entries\n",
    "    print(list_of_entries)\n",
    "\n",
    "    if sort == True:\n",
    "        print('Sort along which axis? Index: ')     \n",
    "        axis = input()\n",
    "        for i in range(1,len(array_in)):           \n",
    "        key_item = float(array_in[i,axis])    \n",
    "        j = i-1\n",
    "        while j >= 0 and abs(float(array_in[j,axis])) > abs(key_item):\n",
    "            array_in[j+1,axis]=float(array_in[j,axis])\n",
    "            j = j-1\n",
    "        array_in[j+1, axis] = key_item\n",
    "\n",
    "        if Path(path).exists() == False:   #check if file exists already, ask before overwrite\n",
    "            np.savetxt(path, array_in, fmt = '%s', header = 'Correlated : x -> y -> Pearson r(x,y) \\n', delimiter = ' -> ', footer = '\\nSorted using insertion sort based on absolute value \\n')\n",
    "        elif Path(path).exists() == True:\n",
    "        print('{} already exists. Overwrite? Y/N'.format(path))\n",
    "        check = input()\n",
    "        if check == 'Y':\n",
    "            np.savetxt(path, array_in, fmt = '%s', header = 'Correlated : x -> y -> Pearson r(x,y) \\n', delimiter = ' -> ', footer = '\\nSorted using insertion sort based on absolute value \\n')\n",
    "            print('File {} was overwriten'.format(path))\n",
    "        else:\n",
    "            print('File {} was not overwriten'.format(path))\n",
    "\n",
    "    else:\n",
    "        if Path(path).exists() == False:   #check if file exists already, ask before overwrite\n",
    "            np.savetxt(path, array_in, fmt = '%s', header = 'Correlated : x -> y -> Pearson r(x,y) \\n', delimiter = ' -> ')\n",
    "        elif Path(path).exists() == True:\n",
    "        print('{} already exists. Overwrite? Y/N'.format(path))\n",
    "        check = input()\n",
    "        if check == 'Y':\n",
    "            np.savetxt(path, array_in, fmt = '%s', header = 'Correlated : x -> y -> Pearson r(x,y) \\n', delimiter = ' -> ')\n",
    "            print('File {} was overwriten'.format(path))\n",
    "        else:\n",
    "            print('File {} was not overwriten'.format(path))\n",
    "\n",
    "get_values(test, 'time', True, '../data/Test1')\n",
    "get_values(test, 'time', True, '../data/Test2')\n",
    "get_values(test, 'time', True, '../data/Test3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the entries in the left column are not repeated if they do not change from the row above (so the fourth feature pair is MO3 and MO6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Print the resulting data to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "np.savetxt('../data/Correlated_npop.txt', sorted_array, fmt = '%s', header = 'Correlated : x -> y -> Pearson r(x,y) \\n', delimiter = ' -> ', footer = '\\nSorted using insertion sort based on absolute value \\n')  #just some fancying around :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Calculate the Euclidean distance (L2 norm) for the vectors in `table.dat`\n",
    "\n",
    "\n",
    "The Euclidean distance measures the distance between to objects that are not points:\n",
    "\n",
    "$d(p,q) = \\sqrt{\\left(p-q\\right)^2}$\n",
    "\n",
    "In this case, consider each of the columns in table.dat as a vector in Euclidean space, where column $r(x)$ and column $v(x)$ denote a pair of vectors that should be compared, as well as $r(y)$ and $v(y)$, and r(z) and v(z).\n",
    "\n",
    "(Background: These are dipole moment components in different gauges, the length and velocity gauge.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in table.dat - I suggest reading it as a numpy array\n",
    "# replace the NaNs by zero -> where are the NaNs? ah, there they are\n",
    "table_data = read_in_df(filedir, filenames[4])\n",
    "table_data=table_data.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate how different the vectors in column 2 are from column 3, column 4 from column 5, and column 6 from column 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate the Euclidean distance\n",
    "def get_vectors(df, x, y):                                  #extract the interesting columns (\"vectors\") into separate dataframes\n",
    "    df_out = pd.DataFrame(index=df.index, columns=[x, y])\n",
    "    df_out[x] = df[x]\n",
    "    df_out[y] = df[y]    \n",
    "    return df_out\n",
    "\n",
    "x_vectors=get_vectors(table_data, 'r(x)', 'v(x)')           \n",
    "y_vectors=get_vectors(table_data, 'r(y)', 'v(y)')\n",
    "z_vectors=get_vectors(table_data, 'r(z)', 'v(z)')\n",
    "\n",
    "def get_distance(vectors):                                  #get distance between vectors by essentially creating a vector which points from one\n",
    "    diff_summ = 0                                           #to the other (difference of input vectors) and than calculating the length of the \n",
    "    for i in range(len(vectors.index)):                     #new vector through (summ_components**2)**(1/2)\n",
    "        diff_summ = diff_summ + (vectors.iloc[i, 0]- vectors.iloc[i, 1])**2\n",
    "    distance = (diff_summ)**(1/2)\n",
    "    return distance\n",
    "\n",
    "distance_x = get_distance(x_vectors)\n",
    "distance_y = get_distance(y_vectors)\n",
    "distance_z = get_distance(z_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the result and save to a .pdf\n",
    "\n",
    "#what, plot the values? just three points... I don't think I got this right\n",
    "plt.plot(['x','y','z'],[distance_x,distance_y,distance_z], 'ob')\n",
    "plt.ylabel('Distance r/v')\n",
    "plt.xlabel('Axis')\n",
    "plt.savefig('plots/The_distances_I_guess.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the result to a file\n",
    "save = np.array([['Distance r(x)/v(x)', distance_x], ['Distance r(y)/v(y)', distance_y], ['Distance r(z)/v(z)', distance_z]])\n",
    "\n",
    "np.savetxt('data/saved_distances_table.txt', save, fmt='%s' , delimiter = ' : ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical analysis\n",
    "\n",
    "Analyze the data using autocorrelation functions and discrete Fourier transforms. Plot your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def read_in_np(filedir, filename):\n",
    "    name = '{}{}'.format(filedir, filename)\n",
    "    print('Reading from file {} - numpy'.format(name))\n",
    "    data = np.loadtxt(name, skiprows=1)\n",
    "    data = data.T\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Read in `efield.t` and Fourier-transform relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efield = read_in_np('data/' , 'efield.t')\n",
    "#print(efield)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are interested in column 2 since the others are constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discard the columns with variance below threshold - these are considered constant\n",
    "\n",
    "efield_y=efield[2]\n",
    "#print(efield_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discrete Fourier transform of the remaining column: You only need the real frequencies\n",
    "signal = np.fft.rfft(efield_y)\n",
    "signal = signal.real\n",
    "n = efield[0].size\n",
    "freq = np.fft.rfftfreq(n) #hier rfft damit beide gleich lang sind mit fft kann nicht geplottet werde\n",
    "print(efield_dft)\n",
    "print(freq)\n",
    "print(efield_dft.size) #überprüfen ob beide gleich lang sind\n",
    "print(freq.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Generate a plot of your results to be saved as pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot your results\n",
    "import matplotlib.pyplot as plt\n",
    "#print(len(efield[0]))\n",
    "#print(len(efield_dft))\n",
    "#plt.hist(efield_dft)\n",
    "#plt.xlabel('Frequenz')\n",
    "#plt.ylabel('Anzahl')\n",
    "#plt.savefig('efield_dft_hist.pdf')\n",
    "plt.plot( freq , efield_dft ) #plot freq gegn signal\n",
    "plt.savefig('efield_dft.pdf') #speichern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Calculate the autocorrelation function from nstate_i.t\n",
    "The autocorrelation function measures how correlated subsequent vectors are with an initial vector; ie. \n",
    "\n",
    "$\\Psi_{corr} = \\langle \\Psi(t=0) | \\Psi(t) \\rangle = \\int_0^{tfin} \\Psi(0)^* \\Psi(t) dt$\n",
    "\n",
    "Since we are in a numerical representation, the integral can be replaced with a sum; and the given vectors are already normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in as numpy array\n",
    "nstate = read_in_np('data/' , 'nstate_i.t')\n",
    "print(nstate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the time column (column 0) in a vector and drop from array \n",
    "time = nstate[0]\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct the data representation: this is in fact a complex matrix\n",
    "# the real part of each matrix column is contained in numpy array column 0, 2, 4, 6, ...\n",
    "# the imaginary part of each matrix column is contained in numpy array column 1, 3, 5, 7, ...\n",
    "# convert the array that was read as dtype=float into a dtype=complex array\n",
    "fct = np.delete(nstate,[0],axis=0) #zeit aus der matrix löschen da hierfür nicht relevant\n",
    "real = fct[0::2]\n",
    "imag = fct[1::2]\n",
    "comp_fct = real + 1j*imag #zusammensetzen der beiden Teile \n",
    "print(comp_fct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the autocorrelation function, we want the overlap between the first vector at time 0 and all \n",
    "# subsequent vectors at later times - the sum of the product of initial and subsequent vectors for each time step\n",
    "\n",
    "#d#def calc_auto(comp_fct):\n",
    "  #  for i in range(0,len(comp_fct[0])):\n",
    "   #     aucofu[i] = np.sum(comp_fct[:,0]*np.conjugate(comp_fct[:,i]))  \n",
    "   #      aucofu = np.append(comp_fct[:,0],dtype = complex) geht irgendwie nicht :( ahhhh wiesooooo\n",
    "    #return aucofu\n",
    "def rechne_autofct(comp_fct):\n",
    "    autofct = np.zeros(len(comp_fct[0]), dtype = complex) #leere (null) matrix mit länge von komplexer \n",
    "    for i in range(0,len(comp_fct[0])):\n",
    "        autofct[i] = np.sum(comp_fct[:,0]*np.conjugate(comp_fct[:,i])) #funktion von oben \n",
    "    return autofct\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autofct = rechne_autofct(comp_fct)\n",
    "print(autofct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Generate a plot of your results to be saved as pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autofct_reel = np.absolute(autofct**2) #da das sonst imaginäre ist \n",
    "plt.plot(autofct_reel)\n",
    "plt.savefig('autofct_plot.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Discrete Fourier transform of the autocorrelation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discrete Fourier-transform the autocorrelation function - now we need all frequency components, \n",
    "# also the negative ones\n",
    "autofct_dft = np.fft.fft(autofct)\n",
    "time_dft = np.fft.fftfreq(len(time))\n",
    "print(time.size)\n",
    "print(autofct_dft.size) #größe überprüfen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Generate a plot of your results to be saved as pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the power spectrum (abs**2)\n",
    "time_reel = np.absolute(time_dft**2)\n",
    "autofct_reel = np.absolute(autofct_dft) #bin nicht sicher ob man das somachen soll...\n",
    "plt.plot(autofct_reel , time_reel)\n",
    "plt.savefig('end_plot.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
